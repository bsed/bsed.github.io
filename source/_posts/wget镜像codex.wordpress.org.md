---
title: "wget镜像codex.wordpress.org"
categories: [ "PHP" ]
tags: [ "wordpress" ]
draft: false
slug: "wget-mirror-codex-wordpress"
date: "2011-06-20 08:19:00"
---

**wget 使用实例一则**

简单说一下,wget是Linux中的一个开源下载工具.貌似没有图像界面.命令行运行.但是功能很强大. 想要详细知道的请自己问google

今天只是记录一下使用wget的一个例子---**镜像 codex.wordpress.org**


<!--more-->


过几天就考试了,等到考试的时候就要静下心来迎接考试了.考试后放假.回到家估计就没有网络了.所以这两天看看把一些重要的东西弄到电脑上带回家看.

wordpress我一直都想学习.但是没有什么好的资料.只好先去看看官方文档了. 没有网络的情况下,看这些文档很不方便.所以我就想把文档镜像到本地.这样回到家后也可以学习了.再搭建个本地php环境.说不定咱还写点东西呢.呵呵

本来在网上搜索了一番,找了windows中一个叫做webzip的软件,试验了一下,感觉不好用.会下载好多文件夹.弄的很乱.好像下载下来的文件名称还会有乱码.所以果断放弃了.然后我又看到了linux中的一个关于wget文章.我记得我原来的时候是用过这个小东西的. 既然我有用linux系统,为什么不尝试一下呢?

又浏览了几个网站,决定使用这个小东西完成我的任务.

    mkdir codex
    cd codex
    wget -r -p -k -np codex.wordpress.org

OK,下载开始了,我要做的就是等待了. 简要说一下几个参数的意思.

- **-r**就是递归下载.也就是将整个网站全部下载下来
- **-p**就是下载网站所需要的一些图片什么的. 我可不想下载下来的网页里面只有文字
- **-k**就是下载完成后转换那些绝对链接为相对链接,适合本地浏览.
- **-np**貌似是不下载父目录,只下载指定网址下的所有网页.

我也搞不懂哪个是 只下载指定网站内链的网页的参数了.但是加上前面几个参数,就行.只下载codex.wordpress.org这个域名后面的网页,对于其他的一些外链,则跳过.要不然,我估计得把整个互联网给下载下来了~~

等了好长好长时间,终于下载完成了.我看了一下,大约体积是150M左右.

然后我就点击一个网页看看.英文网页浏览没问题.中文的出了点问题.在每个中文网页的前面,都会有一个**zh-cn:**的前缀,这个大概就是标注是中文网页的.但是浏览器就不认了.看到那个冒号了没? 浏览器会认为这个zh-cn:是一种新的网络协议,但是浏览器又不知道这个是哪种协议,所以就会找不到网页.

咋办? 改名呗,把每个文件中的这样的链接前面加上一个 **./**就好了. 这个**./**大家应该都知道代表是本目录的意思.这样浏览器就会乖乖的显示网页了.可是文件足足有2000+个,我也不知道哪些文件里面有这样的链接,哪些文件里面没有啊.网上google一番,用正则表达式批量替换.

    sed -i "s/href="zh-cn:/href="./zh-cn:/g" `grep href="zh-cn: -rl /var/www/codex`

将所有的**href="zh-cn:**替换成 **href="./zh-cn:** 即可.

sed加grep 正则表达式轻松搞定.中间加了号来转义"和/.这样点击网页内的带有zh-cn: 这样的链接浏览器就能够正常浏览了.

HOHO~~,完毕,有童鞋想要的没? 看看要的人多的话,俺看看上传到网盘上去

![](/usr/uploads/2011/06/20_01.png)